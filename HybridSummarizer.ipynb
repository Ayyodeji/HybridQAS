{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1"
      ],
      "metadata": {
        "id": "gG6LbUsAHWbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DEPENDENCIES"
      ],
      "metadata": {
        "id": "fha9lC6bsHtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Block all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "EERoDJ_DKaIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "G3eIj31U7ijx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LIBRARY"
      ],
      "metadata": {
        "id": "5KigCvj-sLti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import Summarizer, TransformerSummarizer\n",
        "from transformers import LongformerTokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "t-PgbR383pYg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptUp70oghBL_",
        "outputId": "7d45e790-4f31-4dd7-c7a6-72cc22590877"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PREPROCESSING"
      ],
      "metadata": {
        "id": "j4A07i9tsThL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"/content/drive/My Drive/cnn_dailymail\""
      ],
      "metadata": {
        "id": "deYtRft7htrV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV data as pandas dataframes\n",
        "train_data = pd.read_csv(os.path.join(data, \"train.csv\"))\n",
        "val_data = pd.read_csv(os.path.join(data, \"validation.csv\"))"
      ],
      "metadata": {
        "id": "IokBVSh83jCi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "AVXFHNAh8h10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF9JLkWB9owE",
        "outputId": "fca573ce-f32c-40da-daa0-7f4cf1ad23c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'article', 'highlights'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "truncate = -1\n",
        "df = pd.read_csv(os.path.join(data, \"train.csv\"))[:truncate]\n",
        "df_val = pd.read_csv(os.path.join(data, \"validation.csv\"))[:truncate]\n",
        "df_train = df[[\"article\", \"highlights\"]]\n",
        "df_train.columns = [\"text\", \"summary\"]\n",
        "df_val = df_val[[\"article\", \"highlights\"]]\n",
        "df_val.columns = [\"text\", \"summary\"]"
      ],
      "metadata": {
        "id": "P2eGYdFR99S0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "id": "jwqvNlmPBHax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2"
      ],
      "metadata": {
        "id": "hN5p9mSBNKzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOKENIZER TEST"
      ],
      "metadata": {
        "id": "uWGBLGbfLrB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PHASE 1"
      ],
      "metadata": {
        "id": "GxORoNq0scN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertTokenizer, GPT2Tokenizer, LongformerTokenizer\n",
        "# from summarizer import Summarizer, TransformerSummarizer\n",
        "# import nltk\n",
        "\n",
        "# # Download the NLTK sentence tokenizer data (if not already downloaded)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# # Your input text\n",
        "# body = '''\n",
        "#         Eight people are dead following two shootings at shisha bars in the western German city of Hanau. At least five people were injured after gunmen opened fire at about 22:00 local time (21:00 GMT), police told the BBC. Police added that they are searching for the suspects, who fled the scene and are currently at large. The first shooting was at a bar in the city centre, while the second was in Hanau's Kesselstadt neighbourhood, according to local reports. Police officers and helicopters are patrolling both areas. An unknown number of gunmen killed three people at the first shisha bar, Midnight, before driving to the Arena Bar & Cafe and shooting dead another five victims, regional broadcaster Hessenschau reports. A dark-coloured vehicle was then seen leaving the scene.The motive for the attack is unclear, a police statement said. Can-Luca Frisenna, who works at a kiosk at the scene of one of the shootings said his father and brother were in the area when the attack took place. It's like being in a film, it's like a bad joke, that someone is playing a joke on us, he told Reuters.I can't grasp yet everything that has happened. My colleagues, all my colleagues, they are like my family - they can't understand it either. Hanau, in the state of Hessen, is about 25km (15 miles) east of Frankfurt. It comes four days after another shooting in Berlin, near a Turkish comedy show at the Tempodrom concert venue, which killed one person.\n",
        "#         '''\n",
        "\n",
        "# # BERT tokenizer for extractive part\n",
        "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# # Tokenize the input text for extractive model\n",
        "# inputs_extractive = bert_tokenizer(body, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "# # Step 1: Extractive Summarization using BERT-based model\n",
        "# extractive_model = Summarizer()\n",
        "# extractive_summary = ''.join(extractive_model(body, min_length=60))\n",
        "\n",
        "# # GPT2 tokenizer for abstractive part\n",
        "# gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# # Tokenize the extractive summary for abstractive model\n",
        "# inputs_abstractive = gpt2_tokenizer(extractive_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "# # Longformer tokenizer for compressive part\n",
        "# longformer_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# # Tokenize the abstractive summary for compressive model\n",
        "# inputs_compressive = longformer_tokenizer(abstractive_summary, return_tensors=\"pt\", max_length=4096, truncation=True, padding=True)\n",
        "\n",
        "# # Step 3: Compressive Summarization using sentence scoring with the output of the abstractive model as input\n",
        "# def compression_summary(text, num_sentences=3):\n",
        "#     # Reusing the Longformer tokenizer for compression\n",
        "#     tokenizer = longformer_tokenizer\n",
        "\n",
        "#     # Tokenize the text into sentences using NLTK sentence tokenizer\n",
        "#     sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "#     # Calculate scores for each sentence based on length and position in the text\n",
        "#     scores = [len(sent.strip().split()) / len(text.split()) + i / len(sentences) for i, sent in enumerate(sentences)]\n",
        "\n",
        "#     # Sort sentences based on scores and select the top sentences\n",
        "#     selected_sentences = [sentences[i] for i in sorted(range(len(scores)), key=lambda k: scores[k], reverse=True)[:num_sentences]]\n",
        "\n",
        "#     # Check if the last character of the generated summary is a period and remove it if present\n",
        "#     if selected_sentences[-1].strip().endswith(\".\"):\n",
        "#         selected_sentences[-1] = selected_sentences[-1].strip()[:-1]\n",
        "\n",
        "#     return \" \".join(selected_sentences)\n",
        "\n",
        "# compression_summary_text = compression_summary(abstractive_summary, num_sentences=3)\n",
        "# final_summary = compression_summary_text\n",
        "# # Final Summary: Combine the output from all three stages\n",
        "# print(compression_summary_text)"
      ],
      "metadata": {
        "id": "WVFlaDAKW-MA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PHASE 2"
      ],
      "metadata": {
        "id": "FLd7DAWysgau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import os\n",
        "# from transformers import BertTokenizer, GPT2Tokenizer, LongformerTokenizer\n",
        "# from summarizer import Summarizer, TransformerSummarizer\n",
        "# import nltk\n",
        "\n",
        "# # Download the NLTK sentence tokenizer data (if not already downloaded)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# # Load the pre-trained tokenizers and models\n",
        "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# longformer_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "# extractive_model = Summarizer()\n",
        "\n",
        "# # Define the compression summary function\n",
        "# def compression_summary(text, num_sentences=3):\n",
        "#     # Tokenize the text into sentences using NLTK sentence tokenizer\n",
        "#     sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "#     # Calculate scores for each sentence based on length and position in the text\n",
        "#     scores = [len(sent.strip().split()) / len(text.split()) + i / len(sentences) for i, sent in enumerate(sentences)]\n",
        "\n",
        "#     # Sort sentences based on scores and select the top sentences\n",
        "#     selected_sentences = [sentences[i] for i in sorted(range(len(scores)), key=lambda k: scores[k], reverse=True)[:num_sentences]]\n",
        "\n",
        "#     # Check if the last character of the generated summary is a period and remove it if present\n",
        "#     if selected_sentences[-1].strip().endswith(\".\"):\n",
        "#         selected_sentences[-1] = selected_sentences[-1].strip()[:-1]\n",
        "\n",
        "#     return \" \".join(selected_sentences)\n",
        "\n",
        "# # Define the hybrid summarization pipeline\n",
        "# def hybrid_summarization_pipeline(text):\n",
        "#     # Step 1: Extractive Summarization using BERT-based model\n",
        "#     inputs_extractive = bert_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "#     extractive_summary = extractive_model(text, min_length=60)\n",
        "\n",
        "#     # Step 2: Abstractive Summarization using GPT-2-based model\n",
        "#     inputs_abstractive = gpt2_tokenizer(extractive_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "#     # Step 3: Compressive Summarization using sentence scoring with the output of the abstractive model as input\n",
        "#     compression_summary_text = compression_summary(extractive_summary, num_sentences=3)\n",
        "#     final_summary = compression_summary_text\n",
        "\n",
        "#     return final_summary\n",
        "\n",
        "# # Load the dataset and test the model using the hybrid pipeline\n",
        "# data = \"/content/drive/My Drive/cnn_dailymail\"\n",
        "# truncate = -1\n",
        "# df = pd.read_csv(os.path.join(data, \"train.csv\"))[:truncate]\n",
        "\n",
        "# for index, row in df.iterrows():\n",
        "#     text = row['article']\n",
        "#     summary = row['highlights']\n",
        "\n",
        "#     # Test the hybrid pipeline on the text\n",
        "#     generated_summary = hybrid_summarization_pipeline(text)\n",
        "\n",
        "#     # Print the original summary and the generated summary\n",
        "#     print(\"Original Summary:\", summary)\n",
        "#     print(\"Generated Summary:\", generated_summary)\n",
        "#     print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "l_6XKSpsKH2Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADD DENSE LAYER EXTRACTIVE BERT STEP**\n",
        "##### *FINETUNE*"
      ],
      "metadata": {
        "id": "RpK-bYsoet5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text']"
      ],
      "metadata": {
        "id": "VhdVPDcuiVsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, LongformerTokenizer\n",
        "from summarizer import Summarizer, TransformerSummarizer\n",
        "import nltk\n",
        "\n",
        "# Download the NLTK sentence tokenizer data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the pre-trained tokenizers and models\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "extractive_model = Summarizer()\n",
        "\n",
        "# Define the sentence scoring model on top of BERT\n",
        "class SentenceScorer(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(SentenceScorer, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dense = nn.Linear(768, 1)  # 768 is the hidden size of BERT\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        scores = self.dense(pooled_output)\n",
        "        return scores\n",
        "\n",
        "# Instantiate the sentence scorer model\n",
        "sentence_scorer = SentenceScorer(bert_model)\n",
        "\n",
        "# Define the compression summary function using the sentence scorer\n",
        "def compression_summary(text, num_sentences=3):\n",
        "    # Tokenize the text into sentences using NLTK sentence tokenizer\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Prepare input tensors for BERT\n",
        "    inputs = bert_tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Get sentence scores using the sentence scorer model\n",
        "    with torch.no_grad():\n",
        "        scores = sentence_scorer(input_ids, attention_mask)\n",
        "\n",
        "    # Convert scores to numpy array and normalize between 0 and 1\n",
        "    scores = scores.squeeze().numpy()\n",
        "    scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Sort sentences based on scores and select the top sentences\n",
        "    selected_sentences = [sentences[i] for i in scores.argsort()[::-1][:num_sentences]]\n",
        "\n",
        "    # Check if the last character of the generated summary is a period and remove it if present\n",
        "    if selected_sentences[-1].strip().endswith(\".\"):\n",
        "        selected_sentences[-1] = selected_sentences[-1].strip()[:-1]\n",
        "\n",
        "    # Return the compressed summary and the sentence scores\n",
        "    return \" \".join(selected_sentences), scores\n",
        "\n",
        "# Define the hybrid summarization pipeline\n",
        "def hybrid_summarization_pipeline(text):\n",
        "    # Step 1: Extractive Summarization using BERT-based model\n",
        "    inputs_extractive = bert_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "    extractive_summary = extractive_model(text, min_length=60)\n",
        "\n",
        "    # Step 2: Abstractive Summarization using GPT-2-based model\n",
        "    inputs_abstractive = gpt2_tokenizer(extractive_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "\n",
        "    # Step 3: Compressive Summarization using sentence scoring with the output of the abstractive model as input\n",
        "    compression_summary_text, sentence_scores = compression_summary(extractive_summary, num_sentences=3)\n",
        "    final_summary = compression_summary_text\n",
        "\n",
        "    return final_summary, sentence_scores\n",
        "\n",
        "# Load the dataset and test the model using the hybrid pipeline\n",
        "data = \"/content/drive/My Drive/cnn_dailymail\"\n",
        "truncate = -1\n",
        "df = pd.read_csv(os.path.join(data, \"train.csv\"))[:truncate]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    text = row['article']\n",
        "    summary = row['highlights']\n",
        "\n",
        "    # Test the hybrid pipeline on the text\n",
        "    generated_summary, sentence_scores = hybrid_summarization_pipeline(text)\n",
        "\n",
        "    # Print the original summary and the generated summary\n",
        "    print(\"Original Summary:\", summary)\n",
        "    print(\"Generated Summary:\", generated_summary)\n",
        "    print(\"Sentence Scores:\", sentence_scores)\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "p5qJ8HJkexBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FINE TUNE ABSTRACTIVE GPT STEP"
      ],
      "metadata": {
        "id": "_Sb4GMqdpEjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "from summarizer import Summarizer, TransformerSummarizer\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download the NLTK sentence tokenizer data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the pre-trained tokenizers and models\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "extractive_model = Summarizer()\n",
        "\n",
        "# Define the sentence scoring model on top of BERT\n",
        "class SentenceScorer(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(SentenceScorer, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dense = nn.Linear(768, 1)  # 768 is the hidden size of BERT\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        scores = self.dense(pooled_output)\n",
        "        return scores\n",
        "\n",
        "# Instantiate the sentence scorer model\n",
        "sentence_scorer = SentenceScorer(bert_model)\n",
        "\n",
        "# Define the compression summary function using the sentence scorer\n",
        "def compression_summary(text, num_sentences=3):\n",
        "    # Tokenize the text into sentences using NLTK sentence tokenizer\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Prepare input tensors for BERT\n",
        "    inputs = bert_tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Get sentence scores using the sentence scorer model\n",
        "    with torch.no_grad():\n",
        "        scores = sentence_scorer(input_ids, attention_mask)\n",
        "\n",
        "    # Convert scores to numpy array and normalize between 0 and 1\n",
        "    scores = scores.squeeze().numpy()\n",
        "    scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Sort sentences based on scores and select the top sentences\n",
        "    selected_sentences = [sentences[i] for i in scores.argsort()[::-1][:num_sentences]]\n",
        "\n",
        "    # Check if the last character of the generated summary is a period and remove it if present\n",
        "    if selected_sentences[-1].strip().endswith(\".\"):\n",
        "        selected_sentences[-1] = selected_sentences[-1].strip()[:-1]\n",
        "\n",
        "    # Return the compressed summary and the sentence scores\n",
        "    return \" \".join(selected_sentences), scores\n",
        "\n",
        "# Define the hybrid summarization pipeline\n",
        "def hybrid_summarization_pipeline(text):\n",
        "    # Step 1: Extractive Summarization using BERT-based model\n",
        "    inputs_extractive = bert_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "    extractive_summary = extractive_model(text, min_length=60)\n",
        "\n",
        "    # Step 2: Fine-tune GPT-2 on the extractive summary (target is the summary from the dataset)\n",
        "    gpt2_model.train()\n",
        "    inputs_gpt2 = gpt2_tokenizer(extractive_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "    labels = gpt2_tokenizer(extractive_summary, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)['input_ids']\n",
        "    loss = gpt2_model(**inputs_gpt2, labels=labels).loss\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Step 3: Compressive Summarization using sentence scoring with the output of the fine-tuned GPT-2 model as input\n",
        "    compression_summary_text, sentence_scores = compression_summary(extractive_summary, num_sentences=3)\n",
        "    final_summary = compression_summary_text\n",
        "\n",
        "    return final_summary, sentence_scores\n",
        "\n",
        "# Load the dataset and fine-tune the GPT-2 model on the extractive summaries\n",
        "data = \"/content/drive/My Drive/cnn_dailymail\"\n",
        "truncate = -1\n",
        "df = pd.read_csv(os.path.join(data, \"train.csv\"))[:truncate]\n",
        "\n",
        "# Set up fine-tuning parameters\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "optimizer = AdamW(gpt2_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Fine-tuning loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        text = row['article']\n",
        "        summary = row['highlights']\n",
        "\n",
        "        # Test the hybrid pipeline on the text\n",
        "        generated_summary, sentence_scores = hybrid_summarization_pipeline(text)\n",
        "\n",
        "        # Print the original summary and the generated summary\n",
        "        print(\"Original Summary:\", summary)\n",
        "        print(\"Generated Summary:\", generated_summary)\n",
        "        print(\"Sentence Scores:\", sentence_scores)\n",
        "        print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "XIQIo-tPpN7P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}